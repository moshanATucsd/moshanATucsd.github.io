---
layout: post
title: Variational autoencoders
categories:
tags:
---

This [Video](https://www.youtube.com/watch?v=5WoItGTWV54&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=14&t=0s) from Stanford CS231n explains it really well, and the basic idea is that we want to maximize the log data likelihood, which can be expressed as  

![vae](/images/posts/vae.jpg)

* In the equation above, the first step follows because our training data likelihood does not depend on latent variable, and thus taking the expectation wrt latent variable does not change the log data likelihood.

* In the last step, the first term is the decoder, which is the probability of getting a certain training data conditioned on the latent variable. The second is the KL divergence between the encoder and the prior of latent variable, basically we want the distribution of z we get out of the encoder to be similar to a Gaussian prior.

* The last term, which is the KL divergence of our encoder wrt the true posterior distribution of z given x, is intractable. All we know is that this KL term is non-negative.

* Thus, maximizing the log data likelihood corresponds to maximizing its lower bound, which are the first two terms. The first term should increase, which means we want to reconstruct our training data, while the second term should decrease, which means we want our z distribution to also be Gaussian.

![vae2](/images/posts/vae2.png)

The slide above summarizes the training process.

* First we have some input data x, we can train the encoder to obtain the conditional mean and covariance for z.
* Then we sample z from the Gaussian, and pass it to our decoder network which gives the conditional mean and covariance for x, and we can sample from this to get back x.

![vae3](/images/posts/vae_rapara.jpg)

One issue is how to train the VAE. As explained in a slide in [this video](https://www.youtube.com/watch?v=yFBFl1cLYx8), the mean and std of z are static, but std is also scaled by a random constant generated from the Normal distribution.
